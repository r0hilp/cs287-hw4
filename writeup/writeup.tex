
\documentclass[11pt]{article}

\usepackage{common}
\title{HW4: Word Segmentation}
\author{Jeffrey Ling \\ jling@college.harvard.edu \and Rohil Prasad \\ prasad01@college.harvard.edu }
\begin{document}

\maketitle{}
\section{Introduction}

In this assignment, we examine the problem of word segmentation. Given a string of characters without spaces, we want to determine where we can insert spaces to segment the string into words. 

We implement and discuss several approaches to word segmentation in this paper, all trained on a portion of the Penn Treebank. Our first class of models are hidden Markov models with emission distributions given by an n-gram count model and an adaptation of Bengio's neural network language model (NNLM). Our second class of models are recurrent neural networks (RNNs), namely the Long Short Term Memory (LSTM) network and the Gated Recurrent Unit (GRU) network. Furthermore, we attempt to optimize the evaluation algorithms used to construct segmentations given a model. For the Markovian models, we use a dynamic programming algorithm which greatly improves predictive accuracy. We also make a small adjustment to the RNN evaluation algorithm that the performance of these models as well. 

In Section 2, we give a formal description of our problem and establish our notation. In Section 3, we give detailed descriptions of the algorithms listed above. In Section 4, we present our experimental results. In Section 5, we discuss our findings.

\section{Problem Description}

Assume our training corpus consists of a sequence of characters $c_1, c_2, \dots, c_N$ where $c_i$ is drawn from a vocabulary $\mathcal{V}$ for every $i$. We use $<sp> \in \mathcal{V}$ to denote the space character. 

Our training data represents this corpus as a set of pairs $(c_i, y_i)$ for $i \in \{1, \dots, N\}$. The output variable $y_i$ is set to be equal to $1$ if the next character $c_{i+1}$ is not a space, and $2$ if it is a space. By default, we set $y_N = 1$ since $c_N$ is clearly not followed by a space. 

Calculating a word segmentation is equivalent to taking an input sequence of characters $\mathbf{c}' = c_1c_2\dots c_M$ of characters as input and outputting a sequence $y_1y_2\dots y_M$, where $y_i = 2$ if we insert a space after $c_i$ and $1$ if not. 

We determine the \emph{best} word segmentation by finding the sequence $y_1y_2\dots y_M$ that maximizes the joint probability function $f(c_1, \dots, c_M, y_1, \dots, y_M) = p(c_1, \dots, c_M, y_1, \dots, y_M)$. 

For the approaches we are considering, this requires us to train a model that will take an input character $c$ and output the probability that the next character is a space. Then, we apply an evaluation algorithm at test time to generate the best sequence. 

\subsection{Evaluation}

We evaluate our models on two validation sets. These validation sets contain the same characters, but one already contains spaces while the other does not. 

First, note that our model is essentially a language model, taking in a character and outputting a simple distribution on the next character. Therefore, we can calculate perplexity over the validation set with spaces. This is equal to the exponential of the average negative log likelihood. Since we only have two classes, the perplexity is at most $2$. 

For our second validation set, we can run our model and evaluation algorithm to get a word segmentation over the entire set. Here, we keep track of the number of spaces inserted into each sentence and compute the mean squared error of these numbers with respect to the provided answers for each sentence. 

\section{Model and Algorithms}

\subsection{Markovian Models}

The key to these models is the Markov assumption, which states that $p(y_i | y_1, \dots, y_{i-1}) = p(y_i | y_{i-k+1}, \dots, y_{i-1})$ for some fixed $k$. This allows us to get a simple expression for the joint probability that we can calculate using either a count-based model or a neural network model. 

We can expand out the joint probability as 
\begin{align*}
  p(c_1, \dots, c_M, y_1, \dots, y_M) &= p(c_1, \dots, c_M | y_1, \dots, y_M)p(y_1, \dots, y_M) \\
  &= \prod_{i=1}^M p(c_i | c_1, \dots, c_{i-1}, y_1, \dots, y_M)p(y_i | y_1, \dots, y_{i-1}) \\ 
  &= \prod_{i=1}^M p(c_i | c_1, \dots, c_{i-1}, y_1, \dots, y_M)p(y_i | y_{i-k+1}, \dots, y_{i-1}) \\
\end{align*}

\subsubsection{Count Model}

\subsubsection{Neural Network Linear Model}

\subsection{Recurrent Neural Networks}

\subsubsection{Long Short Term Memory Network}

\subsubsection{Gated Recurrent Unit Network}

\subsection{Evaluation Algorithms}


\section{Experiments}

\section{Conclusion}

\bibliographystyle{apalike}
\bibliography{writeup}

\end{document}
